
# N-LeTaxity: Real-Time NYC Taxi Data Pipeline

## 🧭 Project Goal
Simulate and stream near real-time NYC taxi data using a Python script, and ingest that data into AWS services (S3 and Redshift) for analysis and visualization. The system should start small (1 record/sec) but be designed to scale for high-throughput streaming in the future.

---

## ✅ Architecture Overview

```
╭───────────────────────────╮
│  Your PC / EC2 Instance   │
│  (Python Simulator)       │
│  ─ generate_trip_event()  │
╰────────────┬──────────────╯
             │
             ▼
╭───────────────────────────╮
│ Amazon Kinesis Firehose   │
│ ─ Buffers + Batches Data  │
│ ─ Sends to multiple sinks │
╰──────┬────────────┬───────╯
       │            │
       ▼            ▼
╭────────────╮  ╭────────────────────╮
│   Amazon   │  │     Amazon         │
│     S3     │  │     Redshift       │
│  (Raw Zone)│  │ (Analytics Layer)  │
╰────────────╯  ╰────────────────────╯

    🔍 Future Add-Ons:
    - AWS Lambda (transformation)
    - Kinesis Analytics (real-time SQL)
    - QuickSight/Tableau (dashboards)
```

---

## 📄 Streaming Data Schema (JSON Format)

```json
{
  "trip_id": "cab_1728337",
  "pickup_time": "2025-03-23 12:15:30",
  "dropoff_time": "2025-03-23 12:29:52",
  "PULocationID": 12,
  "DOLocationID": 35,
  "passenger_count": 2,
  "fare_amount": 18.75,
  "payment_type": 1
}
```

---

## 🗂 Project Folder Structure (Planned)

```
N-LeTaxity/
├── scripts/
│   ├── batch/
│   ├── streaming/
│   │   └── streaming_simulator.py
│   └── helpers/
├── infrastructure/
│   ├── glue/
│   ├── emr/
│   ├── kinesis/
│   ├── redshift/
│   └── lambda/
├── analytics/
│   ├── quicksight/
│   └── tableau/
├── sql/
├── data_samples/
├── docs/
│   └── architecture_diagram.png (TBD)
└── README.md
```

---

## 🛠 Task Breakdown (Checklist Style)

### Phase 1: Project Initialization
- [x] Decide project name: `N-LeTaxity`
- [x] Set up project folder structure
- [x] Create GitHub repo and connect local project
- [x] Add README and commit structure

### Phase 2: Build Streaming Simulator
- [x] Define simplified schema for streaming
- [x] Install `boto3`, `faker` on local environment
- [x] Write `streaming_simulator.py` to generate 1 trip/sec
- [x] Connect to AWS Kinesis Firehose
- [ ] Parameterize stream name and message rate
- [ ] Add basic logging and error handling

### Phase 3: AWS Infrastructure
- [x] Create Kinesis Firehose delivery stream
- [ ] Configure destination: S3 (initial), Redshift (next)
- [ ] Verify delivery to S3 bucket (raw data)
- [ ] Create Redshift table for streamed schema
- [ ] Configure Firehose to deliver to Redshift (if used)

### Phase 4: Data Validation
- [ ] Query S3 using Athena or Glue
- [ ] Validate schema and data format
- [ ] Create Glue Catalog table for S3 bucket (optional)

### Phase 5: Visualization (Future)
- [ ] Connect Redshift to QuickSight
- [ ] Create sample dashboard (trip volume, avg fare, etc.)
- [ ] Optional: Add Tableau integration

### Phase 6: Scalability Planning
- [ ] Increase simulator rate (10+ msgs/sec)
- [ ] Migrate simulator to EC2
- [ ] Add multithreading to simulator (optional)
- [ ] Add monitoring (CloudWatch) for Firehose delivery

---

## 📌 Notes
- Use JSON output for simulator to simplify parsing
- Keep trip timestamps realistic (pickup < dropoff, UTC format)
- Leave room for stream transformation via Lambda in the future

---

Let me know when you want to tag a release, create a PR for a module, or automate parts of the pipeline.