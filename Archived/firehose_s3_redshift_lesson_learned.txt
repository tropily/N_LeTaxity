
Lesson Learned: Firehose → S3 → Redshift Streaming Pipeline

Overview
--------
This exercise involved streaming NYC Taxi data via Firehose + Lambda → S3 → Redshift Serverless, and surfaced real-world challenges related to region mismatches, schema alignment, COPY performance, and deduplication.

Pipeline Components
-------------------
- Kinesis Firehose: Delivers streaming data as Parquet to S3
- Lambda: Transforms incoming JSON payload into structured output
- S3: Stores partitioned data by year/month/day/hour
- Redshift Serverless: Ingests Parquet files using COPY command

Key Lessons & Fixes
-------------------

1. Region Mismatch in COPY
--------------------------
Error:
S3 bucket addressed by the query is in a different region from this cluster.

Fixes:
- Verified S3 bucket was in us-east-1 (LocationConstraint = null)
- Explicitly added REGION 'us-east-1' to COPY command
- Ensured COPY path used s3://bucket-name/... (no signed URLs or endpoints)

2. Invalid IAM Role in COPY
---------------------------
Error:
Role needs to be owned by the same account with cluster.

Fix:
- Replaced fake placeholder ARN (123456789012) with actual account's role ARN
- Associated correct IAM role with Redshift namespace using:
  aws redshift-serverless update-namespace --namespace-name ... --iam-roles arn:...

3. Column Mismatch Between Table and File
-----------------------------------------
Error:
Table columns: 12, Data columns: 8

Cause:
- Missing partition columns (year, month, day, hour) in Lambda output

Fix Options:
- Add those fields to the Lambda transformation
- OR: COPY into a staging table, then derive fields in Redshift

4. File Inspection
------------------
Learned:
- Use aws s3 cp to download file:
  aws s3 cp s3://bucket/path/to/file.parquet taxi_test.parquet

- Use Python to inspect:
  import pandas as pd
  df = pd.read_parquet("taxi_test.parquet")
  print(df.dtypes)

5. COPY Performance & Dedup at Scale
------------------------------------
Challenge:
- LEFT JOINs against large final table (billions of rows) is slow

Best Practices:
- Use DISTKEY and SORTKEY on trip_id
- Deduplicate in staging:
  DELETE FROM final_table
  USING staging_dedup
  WHERE final_table.trip_id = staging_dedup.trip_id;

  INSERT INTO final_table
  SELECT ..., EXTRACT(YEAR FROM ...), ...
  FROM staging_dedup;

- Consider using MERGE if supported by Redshift version

Best Practices Summary
----------------------
| Step             | Best Practice                                         |
|------------------|--------------------------------------------------------|
| Firehose         | Write Parquet format to S3                             |
| Lambda Transform | Add partition columns (year, month, etc.)             |
| Redshift COPY    | Use staging table → transform/insert to final         |
| IAM Role         | Must be in same AWS account as Redshift namespace     |
| COPY Schedule    | Automate via Lambda or Step Functions                 |
| Deduplication    | Use UPSERT, MERGE, or dedup in staging before insert  |
| Performance      | Add DISTKEY and SORTKEY for frequent join/filter keys |
